category_value <- df_category_learning %>%
subset(is_practice == 0 & no_response == 0 & repeated_trial == 0) %>%
group_by(PID, chosen_category) %>%
dplyr::summarise(category_cumulative_value = mean(chosen_exemplar_value, na.rm=1)) %>%
merge(all_categories, by="chosen_category")
View(category_value)
category_value <- df_category_learning %>%
subset(is_practice == 0 & no_response == 0 & repeated_trial == 0) %>%
group_by(PID, chosen_category) %>%
dplyr::summarise(category_cumulative_value = mean(chosen_exemplar_value, na.rm=1)) %>%
merge(df_category_memory, all=TRUE)
category_value <- df_category_learning %>%
subset(is_practice == 0 & no_response == 0 & repeated_trial == 0) %>%
group_by(PID, chosen_category) %>%
dplyr::summarise(category_cumulative_value = mean(chosen_exemplar_value, na.rm=1)) %>%
rename(category = "chosen_category") %>%
merge(df_category_memory, all=TRUE)
category_value <- df_category_learning %>%
subset(is_practice == 0 & no_response == 0 & repeated_trial == 0) %>%
group_by(PID, chosen_category) %>%
dplyr::summarise(category_cumulative_value = mean(chosen_exemplar_value, na.rm=1)) %>%
rename(category = "chosen_category")
df_category_memory <- df_category_memory %>%
merge(category_value, all=TRUE)
preprocess_js_data(data_path="../../Tasks/data",
summary_data_path = all_data_path)
data_path = all_data_path
# Load functions
source("Tools/create_csv_from_js_data.R")
source("Tools/create_interactive_csv_from_js_data.R")
source("Tools/create_event_log.R")
source("Tools/preprocess_js_data.R")
source("Tools/plotting.R")
# Figure parameters
preprocess_js_data(data_path="../../Tasks/data",
summary_data_path = all_data_path)
data_path = all_data_path
df <- list(read.csv(paste0(data_path,"/df_category_learning.csv")),
read.csv(paste0(data_path,"/df_category_memory.csv")),
read.csv(paste0(data_path,"/df_size.csv")),
read.csv(paste0(data_path,"/df_rl.csv")),
read.csv(paste0(data_path,"/df_debreif.csv")),
read.csv(paste0(data_path,"/df_events_log.csv")))
names(df) <- c("category_learning", "category_memory", "size", "rl", "debreif", "events_log")
# load raw data
raw_df <- list(read.csv(paste0(data_path,"/Raw_data/raw_category_learning.csv")),
read.csv(paste0(data_path,"/Raw_data/raw_size.csv")),
read.csv(paste0(data_path,"/Raw_data/raw_rl.csv")))
names(raw_df) <- c("category_learning", "size", "rl")
# ==============================================================================
# compute bonus money
# ==============================================================================
final_tally_category <- raw_df$category_learning %>%
subset(!is.na(total_bonus_tally)) %>%
dplyr::select(PID, total_bonus_tally) %>%
rename(category_learning = "total_bonus_tally")
final_tally_size <- raw_df$size %>%
subset(!is.na(size_final_tally)) %>%
dplyr::select(PID, size_final_tally) %>%
rename(size = "size_final_tally")
final_tally_rl <- raw_df$rl %>%
subset(!is.na(total_bonus_tally)) %>%
dplyr::select(PID, total_bonus_tally) %>%
rename(rl = "total_bonus_tally")
bonus <- merge(merge(final_tally_category,final_tally_size, by="PID"),final_tally_rl, by="PID") %>%
mutate(total_bonus = category_learning + size + rl)
#write.csv(bonus %>% dplyr::select(PID, total_bonus), "Data/Bonus/bonus.csv")
# Create a bonus matrix so we could upload a csv of bonus money with worker IDs
# find workers that were not given bonus money yet
subject_log <- read.csv("Data/Subject_log/subject_log.csv", stringsAsFactors = FALSE)
if (exists("subject_log")){
worker_ID <- subject_log[subject_log$ApprovalStatus=="Pending",
c("Actual.Completion.Code","AmazonIdentifier","AmountBonused")]
colnames(worker_ID) <- c("PID","Worker_ID","AmountBonused")
bonus$PID <- as.character(bonus$PID)
bonus <- left_join(worker_ID, bonus, by = "PID")
write.csv(bonus,"Data/Bonus/bonus.csv")
}
# ==============================================================================
# aggregate warnings throughout the tasks
# ==============================================================================
warning_category <- raw_df$category_learning %>%
group_by(PID) %>%
dplyr::summarize(no_response = sum(no_response, na.rm=1),
missed_instruction_checkup = sum(missed_checkup, na.rm=1),
category_memory_warning = sum(did_not_enter_response, na.rm=1),
task = "category_learning")
warning_size <- raw_df$size %>%
group_by(PID) %>%
dplyr::summarize(no_response = sum(no_response, na.rm=1),
missed_instruction_checkup = sum(missed_checkup, na.rm=1),
category_memory_warning = NaN,
task = "size")
warning_rl <- raw_df$rl %>%
group_by(PID) %>%
dplyr::summarize(no_response = sum(no_response, na.rm=1),
missed_instruction_checkup = sum(missed_checkup, na.rm=1),
category_memory_warning = NaN,
task = "rl")
warning_task <- bind_rows(warning_category,warning_size,warning_rl) %>%
gather(no_response:category_memory_warning, key="event", value="n") %>%
subset(n > 0) %>%
add_column(ttype = "", .after = "task")
# load interaction data
warning_int_data <- read.csv(paste0(data_path,"/df_interaction.csv")) %>%
group_by(PID, task, ttype, event) %>%
summarize(n = n())
warning <- bind_rows(warning_task, warning_int_data) %>%
arrange(PID, task, event, ttype, n)
# ==============================================================================
# find outlier subs
# ==============================================================================
blur_focus_criterion = 10
no_response_criterion = 25
missed_instructions_criterion = 10
warning <- warning %>%
mutate(outlier = ifelse(((event == "blur" | event == "focus") & n > blur_focus_criterion) |
(event == "no_response" & n > no_response_criterion) |
(event == "missed_instructions_criterion" & n > missed_instructions_criterion), 1 ,0))
outlier_subs <- as.numeric(unlist(unique(warning[warning$outlier==1, "PID"])))
all_df <- df
for (i in 1:length(df)){
df[[i]] <- df[[i]] %>% subset(!PID %in% outlier_subs)
write.csv(df[[i]], paste0(clean_data_path,"/df_",names(df)[i],".csv"))
}
p1 <- ggplot(df$category_memory %>% mutate(PID = as.factor(PID)),
aes(y=category_value_response,x=category_cumulative_value)) +
stat_smooth(method="lm", se=FALSE, aes(color=PID), size = 0.4) +
stat_smooth(method="lm", color="black") +
#geom_hline(yintercept=0.5, size=line_size, linetype="dashed") +
#geom_vline(xintercept=0, size=line_size, linetype="dashed") +
#scale_y_continuous(expand=c(0,0), breaks=c(0,0.5,1), limits=c(0,1.025)) +
#scale_x_continuous(expand=c(0,0)) +
theme + point_plot_theme +
theme(legend.position = "none") +
labs(y="Catery value response",
x="Cumulative category value",
title="Category memory")
p1
p1 <- ggplot(df$category_memory %>% mutate(PID = as.factor(PID)),
aes(y=category_value_response,x=category_cumulative_value)) +
stat_smooth(method="lm", se=FALSE, aes(color=PID), size = 0.4) +
stat_smooth(method="lm", color="black") +
#geom_hline(yintercept=0.5, size=line_size, linetype="dashed") +
#geom_vline(xintercept=0, size=line_size, linetype="dashed") +
#scale_y_continuous(expand=c(0,0), breaks=c(0,0.5,1), limits=c(0,1.025)) +
#scale_x_continuous(expand=c(0,0)) +
theme + point_plot_theme +
theme(legend.position = "none") +
labs(y="Catery value response",
x="Cumulative category value",
title="Category memory")
if (Save_plots == 1) {
ggsave(filename=sprintf("Plots/%s.%s","Figure5",fig_type),
plot=p1,
width=fig_size[1],
height=fig_size[2])
ggsave(filename=sprintf("Plots/%s.%s","Figure5","png"),
plot=p,
width=fig_size[1],
height=fig_size[2])
}
if (Save_plots == 1) {
ggsave(filename=sprintf("Plots/%s.%s","Figure5",fig_type),
plot=p1,
width=fig_size[1],
height=fig_size[2])
ggsave(filename=sprintf("Plots/%s.%s","Figure5","png"),
plot=p1,
width=fig_size[1],
height=fig_size[2])
}
event_log <- df$events_log
View(event_log)
raw_cat <- df$category_learning
View(raw_cat)
raw_cat <- raw_df$category_learning
block_duration <- df$events_log %>%
fill(block) %>%
fill(index) %>%
group_by(PID, task, block) %>%
dplyr::summarise(length = sum(duration))
block_duration
View(event_log)
View(df)
View(block_duration)
View(df)
View(block_duration)
# general
n_designs = 4
# category learning
n_novel_trials = 270; # should be divided by 10 (number of possible pairings)
n_exemplars_per_cat = 15;
n_cats = n_novel_trials*2/n_exemplars_per_cat; # 36
n_old_trials = n_novel_trials/3; # 90, should be divided by number of blocks
n_total_stims = n_novel_trials * 2; # two new stims in each trial
n_practice_trials = 5;
n_decisions_trials = n_novel_trials + n_old_trials; # we want new trials within a block to be divided by 10 (the number of trial types)
n_blocks = 6;
old_stims_within_a_block = 1; # 1 if we want to limit the old items to be used from the same block
value_groups = c(20, 40, 60, 80); # make sure number of categories per cond divides equally by value groups
value_sd = 20;
n_first_novel_trials = 5; # the minimum of new trials in the beginning of a block
timing_category_learning = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(24, 14, 13, 4, 5)) # isi and iti timings, taken from an exponential distribution, aevraged to 1700 ms.
# size judgement
size_groups = c("S","M","L","XL")
n_size_cats = 20;
n_size_cats_per_group = n_size_cats/length(size_groups);
n_size_exemplars_per_cat = 9;
n_total_size_stims = n_size_cats * n_size_exemplars_per_cat
n_size_decisions_trials = n_total_size_stims/2;
n_size_blocks = 1;
timing_size = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(18, 12, 8, 4, 3)) # isi and iti timings, taken from an exponential distribution, aevraged to 1700 ms.
# RL task
n_rl_trials = 80; # should be divided by 10
n_rl_blocks = 2;
n_rl_trials_per_block = n_rl_trials/n_rl_blocks;
rl_value_groups_hard = c(40, 60);
rl_value_groups_easy = c(20, 80);
rl_value_sd = 20;
rl_percent_mean = 0.4; # percent of mean value group instances (if the value group is 60, then 0.4 of trials will include $60, 0.3 = $40, and 0.3 = $20)
# timing_RL = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(16, 10, 8, 3, 3)) # isi and iti timings, taken from an exponential distribution, averaged to 1700 ms.
timing_RL = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(8, 5, 4, 2, 1)) # isi and iti timings, taken from an exponential distribution, averaged to 1700 ms.
# general
n_designs = 4
# category learning
n_novel_trials = 270; # should be divided by 10 (number of possible pairings)
n_exemplars_per_cat = 15;
n_cats = n_novel_trials*2/n_exemplars_per_cat; # 36
n_old_trials = n_novel_trials/3; # 90, should be divided by number of blocks
n_total_stims = n_novel_trials * 2; # two new stims in each trial
n_practice_trials = 5;
n_decisions_trials = n_novel_trials + n_old_trials; # we want new trials within a block to be divided by 10 (the number of trial types)
n_blocks = 6;
old_stims_within_a_block = 1; # 1 if we want to limit the old items to be used from the same block
value_groups = c(20, 40, 60, 80); # make sure number of categories per cond divides equally by value groups
value_sd = 20;
n_first_novel_trials = 5; # the minimum of new trials in the beginning of a block
timing_category_learning = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(24, 14, 13, 4, 5)) # isi and iti timings, taken from an exponential distribution, aevraged to 1700 ms.
# size judgement
size_groups = c("S","M","L","XL")
n_size_cats = 20;
n_size_cats_per_group = n_size_cats/length(size_groups);
n_size_exemplars_per_cat = 9;
n_total_size_stims = n_size_cats * n_size_exemplars_per_cat
n_size_decisions_trials = n_total_size_stims/2;
n_size_blocks = 1;
timing_size = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(18, 12, 8, 4, 3)) # isi and iti timings, taken from an exponential distribution, aevraged to 1700 ms.
# RL task
n_rl_trials = 80; # should be divided by 10
n_rl_blocks = 2;
n_rl_trials_per_block = n_rl_trials/n_rl_blocks;
rl_value_groups_hard = c(40, 60);
rl_value_groups_easy = c(20, 80);
rl_value_sd = 20;
rl_percent_mean = 0.4; # percent of mean value group instances (if the value group is 60, then 0.4 of trials will include $60, 0.3 = $40, and 0.3 = $20)
# timing_RL = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(16, 10, 8, 3, 3)) # isi and iti timings, taken from an exponential distribution, averaged to 1700 ms.
timing_RL = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(8, 5, 4, 2, 1)) # isi and iti timings, taken from an exponential distribution, averaged to 1700 ms.
n_size_decisions_trials/n_size_blocks
sum(timing_size$reps)
(n_size_decisions_trials/n_size_blocks)/sum(timing_size$reps)
reps_multiplication <- (n_size_decisions_trials/n_size_blocks)/sum(timing_size$reps)
timing_size$reps <- timing_size$reps*reps_multiplication
timing_size$reps
task_folder <- ".../static/"
getwd()
setwd("/Users/nataliebiderman/Dropbox/NatalieFolder/Columbia_University/2.Natalie/Shohamy_Lab/Research/Categories/Experiments/Semantic fMRI Project/Tasks/tools")
all_stims <- read.csv(paste0(task_folder,"Category_learning/Stimuli/Stimuli_list/category_learning_stimuli.csv"))
task_folder <- "../static/"
all_stims <- read.csv(paste0(task_folder,"Category_learning/Stimuli/Stimuli_list/category_learning_stimuli.csv"))
all_stims
practice_stims <- read.csv(paste0(task_folder,"Category_learning/Stimuli/Stimuli_list/practice_category_learning_stimuli.csv"))
practice_stims
sprintf("%s/Category_learning/Design_matrix/category_learning_trials_v%d.csv",task_folder,n)
sprintf("%s/Category_learning/Design_matrix/category_learning_trials_v%d.csv",task_folder,n)
task_folder
sprintf("%s/Category_learning/Design_matrix/category_learning_trials_v%d.csv",task_folder,n)
sprintf("/Category_learning/Design_matrix/category_learning_trials_v%d.csv",n)
sprintf
all_stims <- read.csv(paste0(task_folder,"Category_learning/Stimuli/Stimuli_list/category_learning_stimuli.csv"))
paste0(task_folder,"Category_learning/Task/Design_matrix/category_learning_trials_v",n,".csv")
n=1
paste0(task_folder,"Category_learning/Task/Design_matrix/category_learning_trials_v",n,".csv")
sprintf("%s/Category_learning/Design_matrix/category_learning_trials_v%d.csv",task_folder,n)
sprintf("%sCategory_learning/Design_matrix/category_learning_trials_v%d.csv",task_folder,n)
sprintf("%sCategory_learning/Design_matrix/category_memory_trials_v%d.csv",task_folder,n)
size_stims <- read.csv(paste0(task_folder,"Size_judgement/Task/Stimuli/Stimuli_list/size_stimuli.csv"))
size_stims <- read.csv(paste0(task_folder,"Size_judgement/Stimuli/Stimuli_list/size_stimuli.csv"))
# size judgement
size_groups = c("S","M","L","XL")
n_size_cats = 20;
n_size_cats_per_group = n_size_cats/length(size_groups);
n_size_exemplars_per_cat = 9;
n_total_size_stims = n_size_cats * n_size_exemplars_per_cat
n_size_decisions_trials = n_total_size_stims/2;
n_size_blocks = 1;
timing_size = data.frame(sec = c(1000, 1500, 2000, 3000, 3500), reps = c(18, 12, 8, 4, 3)) # isi and iti timings, taken from an exponential distribution, aevraged to 1700 ms.
if (n_size_decisions_trials/n_size_blocks > sum(timing_size$reps)){
reps_multiplication <- (n_size_decisions_trials/n_size_blocks)/sum(timing_size$reps)
timing_size$reps <- timing_size$reps*reps_multiplication
}
n=1
# load size stims
size_stims <- read.csv(paste0(task_folder,"Size_judgement/Stimuli/Stimuli_list/size_stimuli.csv"))
# remove test items
#size_stims = subset(size_stims, is_test_item==0)
# for each category keep n_size_exemplars_per_cat per category
curr_size_stims <- c()
all_size_cats <- unique(size_stims$category)
for (c in 1:n_size_cats){
curr_cat <- size_stims %>% subset(category == all_size_cats[c]) %>% sample_frac()
curr_cat <- curr_cat[1:n_size_exemplars_per_cat,]
curr_size_stims <- rbind(curr_size_stims, curr_cat)
}
# create trial types (all combinations of size groups)
possible_pair_types <- as.data.frame(t(combn(size_groups, 2))) %>%
rename(left_size_group = "V1", right_size_group = "V2") %>%
bind_rows(data.frame(left_size_group = size_groups, right_size_group = size_groups)) %>%
mutate(pair_type = paste0(left_size_group,"-",right_size_group),
left_size_num = case_when(left_size_group == "S" ~ 1,
left_size_group == "M" ~ 2,
left_size_group == "L" ~ 3,
left_size_group == "XL" ~ 4),
right_size_num = case_when(right_size_group == "S" ~ 1,
right_size_group == "M" ~ 2,
right_size_group == "L" ~ 3,
right_size_group == "XL" ~ 4),
delta_size = right_size_num - left_size_num)
# create the pair types we want to duplicate while considering a somewhat uniform distribution of delta size
repeated_pair_types <- bind_rows(possible_pair_types[possible_pair_types$delta_size==3,],
possible_pair_types[possible_pair_types$delta_size==2,],
possible_pair_types[possible_pair_types$left_size_group=="M" & possible_pair_types$right_size_group=="L",]) %>%
slice(rep(1:n(), 2))
possible_pair_types <- bind_rows(possible_pair_types, repeated_pair_types)
pair_types <- possible_pair_types %>%
slice(rep(1:n(), n_size_decisions_trials/nrow(possible_pair_types)))
size_trials <- data.frame(
trial_num = NaN,
block = sort(rep(1:n_size_blocks, n_size_decisions_trials/n_size_blocks)),
pair_type = pair_types$pair_type,
iti = NaN,
isi = NaN,
high_size_on_left = NaN,
left_size_group = pair_types$left_size_group,
right_size_group = pair_types$right_size_group,
left_size_num = pair_types$left_size_num,
right_size_num = pair_types$right_size_num)
size_trials
# add a vector for high size on the left
high_size_on_left <- sample(rep(c(0,1), nrow(subset(size_trials, left_size_group!=right_size_group))/2),replace=FALSE)
for (i in 1:nrow(size_trials)){
if (size_trials[i, "left_size_group"] != size_trials[i, "right_size_group"] ){
size_trials[i, "high_size_on_left"] = high_size_on_left[1] # assign the first index
high_size_on_left = high_size_on_left[-1] # remove the first index
}
}
# switch position of left and right size groups if default assignment doesn't match high_size_on_left
for (t in 1:nrow(size_trials)){
if ((size_trials[t, "high_size_on_left"] == 0 & size_trials[t, "left_size_num"] > size_trials[t, "right_size_num"]) |
(size_trials[t, "high_size_on_left"] == 1 & size_trials[t, "left_size_num"] < size_trials[t, "right_size_num"])){
left_n = size_trials[t, "left_size_num"]; right_n = size_trials[t, "right_size_num"]
left_s = size_trials[t, "left_size_group"]; right_s = size_trials[t, "right_size_group"]
size_trials[t, "left_size_num"] = right_n
size_trials[t, "right_size_num"] = left_n
size_trials[t, "left_size_group"] = right_s
size_trials[t, "right_size_group"] = left_s
}
size_trials[t, "delta_size"] = size_trials[t, "left_size_num"] - size_trials[t, "right_size_num"]
}
stims_bank = curr_size_stims
for (t in 1:nrow(size_trials)){
# we randomly select a stimulus according to its size group and then remove it from from stims bank
left_stim = sample_frac(subset(stims_bank, size == size_trials[t,"left_size_group"]))[1,]
# for the right stim, we make sure it isn't from the same category as the left stim
right_stim = sample_frac(subset(stims_bank, size == size_trials[t,"right_size_group"] & category != left_stim$category))[1,]
stims_bank = subset(stims_bank, !index %in% c(left_stim$index, right_stim$index))
# assign the stims parameters
size_trials[t, "left_category"] = left_stim$category
size_trials[t, "right_category"] = right_stim$category
size_trials[t, "left_exemplar"] = left_stim$exemplar
size_trials[t, "right_exemplar"] = right_stim$exemplar
size_trials[t, "left_index"] = left_stim$index
size_trials[t, "right_index"] = right_stim$index
size_trials[t, "left_img"] = left_stim$img
size_trials[t, "right_img"] = right_stim$img
size_trials[t, "left_img_path"] = paste0("Stimuli/Size_stims/", left_stim$size, "/", left_stim$category, "/", left_stim$img)
size_trials[t, "right_img_path"] = paste0("Stimuli/Size_stims/", right_stim$size, "/", right_stim$category, "/", right_stim$img)
}
# we need to add the functions for computing the timing.
for (b in 1:n_size_blocks){
# shuffle order of trials
size_trials[size_trials$block==b,] = sample_frac(size_trials[size_trials$block==b,])
# compute isi and iti
isi = sample(rep(timing_size$sec, timing_size$reps), replace = FALSE)
iti = sample(rep(timing_size$sec, timing_size$reps), replace = FALSE)
size_trials[size_trials["block"]==b, "isi"] = isi
size_trials[size_trials["block"]==b, "iti"] = iti
}
size_trials[,"trial_num"] = 0:(nrow(size_trials)-1)
View(size_trials)
View(size_trials)
for (n in 1:n_designs){
# load size stims
size_stims <- read.csv(paste0(task_folder,"Size_judgement/Stimuli/Stimuli_list/size_stimuli.csv"))
# remove test items
#size_stims = subset(size_stims, is_test_item==0)
# for each category keep n_size_exemplars_per_cat per category
curr_size_stims <- c()
all_size_cats <- unique(size_stims$category)
for (c in 1:n_size_cats){
curr_cat <- size_stims %>% subset(category == all_size_cats[c]) %>% sample_frac()
curr_cat <- curr_cat[1:n_size_exemplars_per_cat,]
curr_size_stims <- rbind(curr_size_stims, curr_cat)
}
# ------------------------------------------------------------------------------
# create trials
# ------------------------------------------------------------------------------
# create trial types (all combinations of size groups)
possible_pair_types <- as.data.frame(t(combn(size_groups, 2))) %>%
rename(left_size_group = "V1", right_size_group = "V2") %>%
bind_rows(data.frame(left_size_group = size_groups, right_size_group = size_groups)) %>%
mutate(pair_type = paste0(left_size_group,"-",right_size_group),
left_size_num = case_when(left_size_group == "S" ~ 1,
left_size_group == "M" ~ 2,
left_size_group == "L" ~ 3,
left_size_group == "XL" ~ 4),
right_size_num = case_when(right_size_group == "S" ~ 1,
right_size_group == "M" ~ 2,
right_size_group == "L" ~ 3,
right_size_group == "XL" ~ 4),
delta_size = right_size_num - left_size_num)
# create the pair types we want to duplicate while considering a somewhat uniform distribution of delta size
repeated_pair_types <- bind_rows(possible_pair_types[possible_pair_types$delta_size==3,],
possible_pair_types[possible_pair_types$delta_size==2,],
possible_pair_types[possible_pair_types$left_size_group=="M" & possible_pair_types$right_size_group=="L",]) %>%
slice(rep(1:n(), 2))
possible_pair_types <- bind_rows(possible_pair_types, repeated_pair_types)
pair_types <- possible_pair_types %>%
slice(rep(1:n(), n_size_decisions_trials/nrow(possible_pair_types)))
size_trials <- data.frame(
trial_num = NaN,
block = sort(rep(1:n_size_blocks, n_size_decisions_trials/n_size_blocks)),
pair_type = pair_types$pair_type,
iti = NaN,
isi = NaN,
high_size_on_left = NaN,
left_size_group = pair_types$left_size_group,
right_size_group = pair_types$right_size_group,
left_size_num = pair_types$left_size_num,
right_size_num = pair_types$right_size_num)
# add a vector for high size on the left
high_size_on_left <- sample(rep(c(0,1), nrow(subset(size_trials, left_size_group!=right_size_group))/2),replace=FALSE)
for (i in 1:nrow(size_trials)){
if (size_trials[i, "left_size_group"] != size_trials[i, "right_size_group"] ){
size_trials[i, "high_size_on_left"] = high_size_on_left[1] # assign the first index
high_size_on_left = high_size_on_left[-1] # remove the first index
}
}
# switch position of left and right size groups if default assignment doesn't match high_size_on_left
for (t in 1:nrow(size_trials)){
if ((size_trials[t, "high_size_on_left"] == 0 & size_trials[t, "left_size_num"] > size_trials[t, "right_size_num"]) |
(size_trials[t, "high_size_on_left"] == 1 & size_trials[t, "left_size_num"] < size_trials[t, "right_size_num"])){
left_n = size_trials[t, "left_size_num"]; right_n = size_trials[t, "right_size_num"]
left_s = size_trials[t, "left_size_group"]; right_s = size_trials[t, "right_size_group"]
size_trials[t, "left_size_num"] = right_n
size_trials[t, "right_size_num"] = left_n
size_trials[t, "left_size_group"] = right_s
size_trials[t, "right_size_group"] = left_s
}
size_trials[t, "delta_size"] = size_trials[t, "left_size_num"] - size_trials[t, "right_size_num"]
}
# ------------------------------------------------------------------------------
# assign stims to trials
# ------------------------------------------------------------------------------
stims_bank = curr_size_stims
for (t in 1:nrow(size_trials)){
# we randomly select a stimulus according to its size group and then remove it from from stims bank
left_stim = sample_frac(subset(stims_bank, size == size_trials[t,"left_size_group"]))[1,]
# for the right stim, we make sure it isn't from the same category as the left stim
right_stim = sample_frac(subset(stims_bank, size == size_trials[t,"right_size_group"] & category != left_stim$category))[1,]
stims_bank = subset(stims_bank, !index %in% c(left_stim$index, right_stim$index))
# assign the stims parameters
size_trials[t, "left_category"] = left_stim$category
size_trials[t, "right_category"] = right_stim$category
size_trials[t, "left_exemplar"] = left_stim$exemplar
size_trials[t, "right_exemplar"] = right_stim$exemplar
size_trials[t, "left_index"] = left_stim$index
size_trials[t, "right_index"] = right_stim$index
size_trials[t, "left_img"] = left_stim$img
size_trials[t, "right_img"] = right_stim$img
size_trials[t, "left_img_path"] = paste0("Stimuli/Size_stims/", left_stim$size, "/", left_stim$category, "/", left_stim$img)
size_trials[t, "right_img_path"] = paste0("Stimuli/Size_stims/", right_stim$size, "/", right_stim$category, "/", right_stim$img)
}
# ------------------------------------------------------------------------------
# add timing variables for each block (run) and shuffle order of trials within a block
# ------------------------------------------------------------------------------
# we need to add the functions for computing the timing.
for (b in 1:n_size_blocks){
# shuffle order of trials
size_trials[size_trials$block==b,] = sample_frac(size_trials[size_trials$block==b,])
# compute isi and iti
isi = sample(rep(timing_size$sec, timing_size$reps), replace = FALSE)
iti = sample(rep(timing_size$sec, timing_size$reps), replace = FALSE)
size_trials[size_trials["block"]==b, "isi"] = isi
size_trials[size_trials["block"]==b, "iti"] = iti
}
# add trial number
size_trials[,"trial_num"] = 0:(nrow(size_trials)-1)
# save data frame
write.csv(size_trials,sprintf("%sSize_judgement/Design_matrix/size_trials_v%d.csv",task_folder,n), row.names = FALSE, fileEncoding = "UTF-8")
}
View(block_duration)
